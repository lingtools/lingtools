#!/usr/bin/env python

"""
Output cohort entropy information for aligned stimuli. The aligned
stimuli should be contained in a directory, each file named
<word>.TextGrid, where <word> is the orthographic form of the
item. The entropy file is the file ending in _prefix.csv generated by
cohort_info. If rate is specified, the output is a long format
containing the entropy at intervals spaced 1.0 / rate seconds
apart. If not specified, the output is in a shorter format containing
a single row per phoneme, including start and end times.

"""

# Copyright 2013-2014 Constantine Lignos
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import csv
import argparse

from lingtools.phon.textgrid import TextGrid
from lingtools.phon.arpabet import arpabet_elpone
from extract_elp_prons import replace_phons

PHONEME_TIER_NAMES = set(("phonemes", "phones"))


def phoneme_tier(textgrid):
    """Return the first tier corresponding to phonemes in a TextGrid."""
    try:
        return next(tier for tier in textgrid
                    if tier.name.lower() in PHONEME_TIER_NAMES)
    except StopIteration:
        return None


def textgrid_files(dirpath):
    """Return a list of files in a directory with the extension "TextGrid"."""
    return [os.path.join(dirpath, filename) for filename in os.listdir(dirpath)
            if filename.lower().endswith(".textgrid")]


def align_cohort(input_dir, ent_path, rate, output_path, arpabet):
    """Output info for each aligned item at the given rate."""
    # Words to their phoneme tiers
    word_phon_tiers = {}

    # Load the textgrids
    textgrid_paths = textgrid_files(input_dir)
    for textgrid_path in textgrid_paths:
        textgrid = TextGrid(textgrid_path)
        textgrid.read(textgrid_path)
        phon_tier = phoneme_tier(textgrid)
        if not phon_tier:
            print "Could not read phoneme tier, skipping {}".format(
                textgrid_path)
            continue
        word = os.path.splitext(os.path.basename(textgrid_path))[0]
        word_phon_tiers[word] = phon_tier

    # Prefixes to their entropy information
    prefix_ent = {}

    # Load entropy
    with open(ent_path, 'rU') as ent_file:
        reader = csv.DictReader(ent_file)
        for row in reader:
            prefix_ent[row['prefix']] = \
                (row['ent.unweight'], row['ent.freq'])

    # Output information at each sample
    output_file = open(output_path, 'wb')
    writer = csv.writer(output_file)
    if not rate:
        # Short output
        writer.writerow(['word', 'position', 'phoneme', 'start', 'end',
                         'ent.unweight', 'ent.freq'])
        for word, tier in word_phon_tiers.iteritems():
            prefix = ""
            for idx, interval in enumerate(tier):
                mark = (replace_phons(interval.mark) if not arpabet else
                        arpabet_elpone([interval.mark])[0])
                prefix += mark
                # Get the entropy
                try:
                    ent_unweight, ent_freq = prefix_ent[prefix]
                except KeyError:
                    ent_unweight, ent_freq = 0, 0
                writer.writerow([word, idx + 1, mark,
                                 interval.minTime, interval.maxTime,
                                 ent_unweight, ent_freq])
    else:
        # Long output
        time_step = 1.0 / rate
        writer.writerow(['word', 'time', 'position', 'phoneme', 'ent.unweight',
                         'ent.freq'])
        for word, tier in word_phon_tiers.iteritems():
            mark = (replace_phons(tier[0].mark) if not arpabet else
                    arpabet_elpone([tier[0].mark])[0])
            prefix = mark
            time = 0
            idx = 0
            # This loop could be optimized easily to speed output on
            # high sample rates
            while True:
                interval = tier[idx]
                maxtime = interval.maxTime
                if time > maxtime:
                    idx += 1
                    try:
                        interval = tier[idx]
                    except IndexError:
                        break
                    mark = (replace_phons(interval.mark)if not arpabet else
                            arpabet_elpone([interval.mark])[0])
                    prefix += mark

                # Get the entropy
                try:
                    ent_unweight, ent_freq = prefix_ent[prefix]
                except KeyError:
                    ent_unweight, ent_freq = 0, 0
                writer.writerow([word, time, idx + 1, mark,
                                 ent_unweight, ent_freq])
                # Step forward
                time += time_step

    output_file.close()


def main():
    """Parse arguments and call the cohort computer."""
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('input_dir', help='directory containing TextGrid files')
    parser.add_argument('prefix_entropy',
                        help='CSV of prefix entropy information')
    parser.add_argument('output', help='output CSV file')
    parser.add_argument('rate', nargs='?', default=None,
                        type=int, help='resolution of output, in Hz')
    parser.add_argument('-a', '--arpabet', action='store_true',
                        help='convert alignments from ARPABET')
    args = parser.parse_args()
    align_cohort(args.input_dir, args.prefix_entropy, args.rate, args.output,
                 args.arpabet)


if __name__ == "__main__":
    main()
